# ğŸ›¡ Project Aegis: Autonomous Data Immunity

**Self-Healing Data Quality System with Auto-Detection and Auto-Repair**

![CI Status](https://github.com/madhavanbalaji02/project-aegis/workflows/CI%20-%20Gatekeeper/badge.svg)
[![Render](https://img.shields.io/badge/Deploy-Render-46E3B7?style=for-the-badge)](https://render.com)
[![Python](https://img.shields.io/badge/Python-3.11-blue?style=for-the-badge&logo=python)](https://python.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io)

---

## ğŸš€ Hybrid Architecture

> **This project uses a Hybrid Architecture designed for dual environments:**
> 
> - **â˜ï¸ Cloud Mode (Render):** Process small files (up to 200MB) with a lightweight 512MB RAM footprint
> - **ğŸ’» Local Mode (Mac/Linux):** Unlock **100GB+ processing capabilities** using DuckDB's zero-copy streaming

**The live demo processes small files. Clone the repo to unlock massive-scale data processing on local hardware!**

---

## ğŸ¯ What is Project Aegis?

Project Aegis is a **fault-tolerant data quality system** that:

âœ… **Auto-detects** data drift and quality issues using Evidently AI  
âœ… **Auto-fixes** data errors with intelligent healing strategies  
âœ… **Scales seamlessly** from megabytes (cloud) to terabytes (local)  
âœ… **Streams efficiently** using DuckDB to avoid memory overflow  

### Key Features

ğŸ” **Drift Detection**
- Calculates drift scores using Evidently AI
- Identifies columns with significant distribution changes
- Analyzes missing value patterns

ğŸ›¡ **Self-Healing Logic**
- **Drop** columns with >50% missing values
- **Impute** columns with <5% missing values using median
- **Flag** columns with 5-50% missing values for manual review

ğŸ’» **Big Data Optimization**
- Uses DuckDB for zero-copy CSV processing
- Samples 100k rows for analysis without loading full dataset
- In-place transformations for minimal memory usage

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Mission Control (app.py)        â”‚
â”‚         Streamlit Dashboard              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Logic Engine (engine.py)          â”‚
â”‚         AegisEngine Class                â”‚
â”‚  â€¢ scan_for_drift()                      â”‚
â”‚  â€¢ heal_data()                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DuckDB   â”‚    â”‚ Evidently   â”‚
â”‚ Streamingâ”‚    â”‚ AI Reports  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.11+
- 8GB+ RAM (for local big data mode)

### Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd project-aegis

# Install dependencies
pip install -r requirements.txt

# Run the application
streamlit run app.py
```

---

## ğŸ–¥ Usage

### Cloud Mode (Upload Files)
1. Navigate to the live demo on Render
2. Upload a CSV file (max 200MB)
3. Click "ğŸš€ Scan for Drift"
4. Review drift metrics and missing value analysis
5. Click "ğŸ”§ AUTO-FIX DATA" if drift > 30%
6. Download the cleaned dataset

### Local Mode (Big Data)
1. Run `streamlit run app.py` locally
2. Select "Local Path" in the sidebar
3. Enter the full path to your large CSV file
4. The system will automatically use DuckDB streaming
5. Cleaned data will be saved to disk with the path displayed

---

## ğŸ§ª Testing with Chaos Monkey

Generate a 1GB corrupted dataset for testing:

```bash
python tests/chaos_monkey.py
```

This creates a massive CSV with intentional errors:
- Random missing values (0-80% per column)
- Mixed data types
- Outliers and anomalies

---

## ğŸ›  Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Frontend** | Streamlit | Interactive dashboard |
| **Backend** | Python 3.11 | Core logic |
| **Data Engine** | DuckDB | Zero-copy SQL analytics |
| **Quality Analysis** | Evidently AI | Drift detection & reporting |
| **Data Processing** | Polars | High-performance DataFrame ops |
| **Visualization** | Plotly | Interactive charts |
| **Deployment** | Render | Cloud hosting |

---

## ğŸ“Š How It Works

### 1. Drift Detection
```python
engine = AegisEngine()
result = engine.scan_for_drift('data.csv')
print(f"Drift Score: {result['drift_score']:.2%}")
```

**Local Mode Optimization:**
```sql
-- DuckDB samples 100k rows without loading full file
SELECT * FROM read_csv_auto('50GB_file.csv') 
USING SAMPLE 100000 ROWS
```

### 2. Auto-Healing
```python
healing_result = engine.heal_data('data.csv')
print(f"Columns dropped: {len(healing_result['columns_dropped'])}")
print(f"Columns imputed: {len(healing_result['columns_imputed'])}")
```

---

## ğŸš€ Deployment

### Deploy to Render

1. Fork/clone this repository
2. Create a new Web Service on [Render](https://render.com)
3. Connect your GitHub repo
4. Render will automatically detect `render.yaml`
5. Deploy! ğŸ‰

**Environment Variables (auto-configured):**
- `RENDER=true` (automatically set by Render)
- `PORT` (assigned by Render)

---

## ğŸ“ˆ Performance Benchmarks

| File Size | Mode | Memory Usage | Processing Time |
|-----------|------|--------------|-----------------|
| 10 MB | Cloud | ~100 MB | ~5 seconds |
| 200 MB | Cloud | ~450 MB | ~30 seconds |
| 1 GB | Local | ~200 MB | ~15 seconds |
| 50 GB | Local | ~500 MB | ~2 minutes |

*Local mode uses DuckDB sampling for constant memory footprint*

---

## ğŸ”’ Memory Safety

### Cloud Mode Protections
- File size limit: 200MB
- Automatic chunking for large uploads
- Memory monitoring
- Graceful degradation

### Local Mode Advantages
- Streaming from disk
- Zero-copy operations
- Sample-based analysis
- In-place transformations

---

## ğŸ¯ Use Cases

âœ… **Data Pipeline Monitoring**: Auto-detect drift in production data  
âœ… **Data Quality Assurance**: Identify and fix issues before analysis  
âœ… **ETL Validation**: Ensure data integrity after transformations  
âœ… **Machine Learning**: Detect training/serving skew  
âœ… **Compliance**: Automated data quality reporting  

---

## ğŸ›£ Roadmap

- [ ] Add support for Parquet and JSON formats
- [ ] Real-time drift monitoring with Watchdog
- [ ] Custom healing strategies via UI
- [ ] Email alerts for critical drift
- [ ] Integration with Apache Airflow
- [ ] Multi-file batch processing

---

## ğŸ“„ License

MIT License - feel free to use for personal and commercial projects!

---

## ğŸ™ Acknowledgments

Built with:
- [DuckDB](https://duckdb.org/) - Blazing fast analytics
- [Evidently AI](https://evidentlyai.com/) - ML monitoring
- [Streamlit](https://streamlit.io/) - Beautiful dashboards
- [Polars](https://pola.rs/) - Lightning fast DataFrames

---

## ğŸ“ Support

For issues, questions, or feature requests, please open an issue on GitHub.

---

<div align="center">
  
**ğŸ›¡ Project Aegis - Autonomous Data Immunity ğŸ›¡**

*"Data errors are inevitable. Recovery is automated."*

</div>
